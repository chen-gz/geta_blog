---
title: Recurrent Neural Network
date: 2024-08-31
categories: [NLP]
tags: [nlp]     # TAG names should always be lowercase
math: true
mermaid: false
---

In this series of posts related to NLP, we will begin by discussing several foundational concepts and basic methodologies used to address the problem. Subsequently, we will cover the latest methods, such as GPTs. 

I presume the audience is familiar with the fundamental concepts of neural networks and multi-layer perceptrons (MLP).

## Word Embedding

Computer-based methods operate on numerical data. Therefore, it is necessary to find a way to transform "word" in the phonetic alphabet or "character" in hieroglyphic languages into numbers.

A one-hot vector is a straightforward concept where a vector consists of a single element set to 1 and all other elements set to 0 to represent a word or character.
A one-hot vector essentially functions like a word index.
However, the issue with a one-hot vector is its sparsity. The vector is so sparse that it makes most methods difficult to process. Therefore, we need a method to map this one-hot vector (index) to an embedding (a vector).

Certain neural network models feature an embedding layer that can be trained during the neural network's training process. Pretrained embedding models like Word2Vec and GloVe are also available for direct use.

### How to Get Word Embedding

**Futher Reading**
* https://arxiv.org/pdf/1301.3781

The primary idea is to construct a network aimed at forecasting the missing word. For illustration, take the sentence 'I eat apple.' If we remove 'apple', converting the sentence to 'I eat __', the network would predict the missing word. Through this approach, we generate the training dataset without requiring any labels.

We shall provide a formal description. Given a dictionary that contains $n$ words, our aim is to develop a model that maps the one-hot encoding of a word to an embedding space $R^d$.
By using the embedded word as the input of the network, the network predicts the omitted word. Hence, we obtain the word embedding by training the network.

**Comments: Word embedding is similar to dimensionality reduction. Therefore, techniques such as t-SNE and UMAP can be viewed as approaches to obtain word embeddings. However, these methods cannot be applied directly, as one-hot vectors do not encapsulate the relationships between words. Nonetheless, if we can map one-hot vectors to a feature space without using a neural network, it might be possible to develop methods that offer greater human interpretability**.


## Recurrent Neural Network (RNN)

Recurrent Neural Networks (RNNs) are a type of neural network architecture that allows for the reuse of the same network across different time steps. This makes them particularly useful for tasks that involve sequential data, such as natural language processing and time series analysis.

The basic idea behind RNNs is to maintain a hidden state that captures information from previous time steps and combines it with the current input to generate an output. This hidden state serves as a memory that allows the network to capture dependencies and patterns in the sequential data.

The architecture of an RNN is illustrated in the following diagram:

![RNN Architecture](https://example.com/rnn_architecture.png)

At each time step, the RNN takes an input vector, denoted as $x_t$, and combines it with the hidden state from the previous time step, denoted as $h_{t-1}$. This combination is typically done using a weighted sum, where the weights are learned during the training process. The resulting hidden state, denoted as $h_t$, is then used to generate the output at that time step, denoted as $y_t$.

The equations that govern the behavior of an RNN can be summarized as follows:

$$
h_t = f(W_{hh}h_{t-1} + W_{hx}x_t) 
$$

$$
y_t = f(W_{yh}h_t)
$$

In these equations, $f$ represents the activation function, which is typically a sigmoid or a hyperbolic tangent function. $W_{hh}$, $W_{hx}$, and $W_{yh}$ are the weight matrices that are learned during the training process.

It is important to note that RNNs have a limitation known as the "vanishing gradient" problem, which makes it difficult for them to capture long-term dependencies in the data. This means that the output at a given time step is primarily influenced by the input at that time step and the previous hidden state, rather than inputs from earlier time steps. However, there are variations of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), that have been developed to address this issue.

RNNs have a wide range of applications, including language translation, text generation, sentiment analysis, and speech recognition. They are particularly effective when dealing with sequential data that exhibits temporal dependencies.

### Training an RNN

To train an RNN, we typically formulate the problem as a classification or regression task and use appropriate loss functions, such as cross-entropy or mean squared error, to measure the difference between the predicted output and the ground truth. The weights of the RNN are then updated using gradient-based optimization algorithms, such as backpropagation through time (BPTT) or truncated backpropagation through time (TBPTT).

Training an RNN can be challenging due to the vanishing gradient problem and the need to handle variable-length sequences. Techniques such as gradient clipping, regularization, and batch normalization can be employed to mitigate these challenges and improve the training process.

In summary, RNNs are a powerful tool for modeling sequential data and capturing temporal dependencies. They have been widely used in various domains and have paved the way for more advanced architectures, such as Transformers and GPTs, in the field of natural language processing.

### Further Reading on RNN

Here are some online resources you can explore for further reading on Recurrent Neural Networks (RNNs):

- [Recurrent Neural Networks - Stanford University](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
- [Understanding LSTM Networks - Christopher Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs)
- [The Unreasonable Effectiveness of Recurrent Neural Networks - Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness)
- [Deep Learning Book - Chapter 10: Sequence Modeling: Recurrent and Recursive Nets](https://www.deeplearningbook.org/contents/rnn.html)
- [Sequence to Sequence Learning with Neural Networks - Ilya Sutskever, Oriol Vinyals, and Quoc V. Le](https://arxiv.org/abs/1409.3215)

<!-- ## Recurrent Neural Network (RNN)

The basic idea of RNN is reuse the same neural network. The process is kind of similar to Morkov Chain. Each state depends on current input and previous state. 

The RNN architecuture is shown in Fig.

![RNN](https://minio.ggeta.com/blog-public-data/Screenshot%20from%202024-08-31%2013-58-18.png)

Each $h_t$ is the hidden state. The hidden state is used to store the information from previous state. $x_t$ is the input at time $t$. $y_t$ is the output at time $t$. 

The formula of RNN is shown as follows:
$$
h_t = f(W_{hh}h_{t-1} + W_{hx}x_t) 
$$

$$
y_t = f(W_{yh}h_t)
$$

The $f$ is the activation function, usually is sigmoid or tanh. The weight $W_{hh}, W_{hx}, W_{yh}$ are the weight that used to train the network.

The RNN can be used for many tasks, such as translation, text generation, etc.

**Comments: The RNN can not handle the long dependency. That means the output at time $t$ is only depend on the input at time $t$ and previous hidden state.**

To use RNN, we need to formula the problem as classification or regression problem, thus we have loss functions to train the network.

CBOW which introduce in last section is a formulated classification problem.

So we can used RNN as a network archtecture get the word embedding. 

Other than word embedding, the RNN can be used of a lot of tasks, for example, langauge translation. 

### Training
    I will write how to train RNN in the future.


## Temperature Sigmoid

Temperature is an important parater when talking about the sequence to sequence model. 
So, we first introduce it here before we go to the transformer.

In a tradtitonal lassification network, we use cross entropy to train the network and we hope the network confident for the class of the input image. 

The classification also used to train the sequence to sequence model. But when we want the model generate more creative content, we need the model generate the value other than the highest probability word. 

### How to Generate a word

In traditional classification network, we generate the tag use top1 or top5 tags. 
But if we use top1 in RNN, that everytime the generate sequence will be fix. That's say, one input only corresspond to one output.
But sometime, we want the model generate more than one results. For example, translation English to Chinese, there are more than one way to translate. 
The way to do this, is we use the probability of prediction and sample the word according to the probability. 
For example, the probability predict of word from a model after "I ate " is that apple: 0.1, orange: 0.8, bananay: 0.1; 
Instead generate orange diretely, we have 10%  probabilty to generate apple and banana, and 80% probability to generate orange.

As we known that, after training, the network have preference to generate some word, that was used to train the network.
So we use temeperature to make the netowrk more creative. 

Here is the way:
When calcualte the probability using sigmoid function $\frac{e^z}{1 + e^z}$, we replace it with  $\frac{e^{z/T}}{1 + e^{z/T}}$, where increase the temperature will increase the entorpy.

This simple mothod can encourage the network generate creative result. 



 -->
