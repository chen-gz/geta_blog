---
title: Recurrent Neural Network
date: 2024-08-31
categories: [NLP]
tags: [nlp]     # TAG names should always be lowercase
math: true
mermaid: false
---

In this series of posts related to NLP, we will begin by discussing several foundational concepts and basic methodologies used to address the problem. Subsequently, we will cover the latest methods, such as GPTs. 

I presume the audience is familiar with the fundamental concepts of neural networks and multi-layer perceptrons (MLP).

## Word Embedding

Computer-based methods operate on numerical data. Therefore, it is necessary to find a way to transform "word" in the phonetic alphabet or "character" in hieroglyphic languages into numbers.

A one-hot vector is a straightforward concept where a vector consists of a single element set to 1 and all other elements set to 0 to represent a word or character.
A one-hot vector essentially functions like a word index.
However, the issue with a one-hot vector is its sparsity. The vector is so sparse that it makes most methods difficult to process. Therefore, we need a method to map this one-hot vector (index) to an embedding (a vector).

Certain neural network models feature an embedding layer that can be trained during the neural network's training process. Pretrained embedding models like Word2Vec and GloVe are also available for direct use.

### How to Get Word Embedding

**Futher Reading**
* https://arxiv.org/pdf/1301.3781

The primary idea is to construct a network aimed at forecasting the missing word. For illustration, take the sentence 'I eat apple.' If we remove 'apple', converting the sentence to 'I eat __', the network would predict the missing word. Through this approach, we generate the training dataset without requiring any labels.

We shall provide a formal description. Given a dictionary that contains $n$ words, our aim is to develop a model that maps the one-hot encoding of a word to an embedding space $R^d$.
By using the embedded word as the input of the network, the network predicts the omitted word. Hence, we obtain the word embedding by training the network.

**Comments: Word embedding is similar to dimensionality reduction. Therefore, techniques such as t-SNE and UMAP can be viewed as approaches to obtain word embeddings. However, these methods cannot be applied directly, as one-hot vectors do not encapsulate the relationships between words. Nonetheless, if we can map one-hot vectors to a feature space without using a neural network, it might be possible to develop methods that offer greater human interpretability**.



<!-- ## Recurrent Neural Network (RNN)

The basic idea of RNN is reuse the same neural network. The process is kind of similar to Morkov Chain. Each state depends on current input and previous state. 

The RNN architecuture is shown in Fig.

![RNN](https://minio.ggeta.com/blog-public-data/Screenshot%20from%202024-08-31%2013-58-18.png)

Each $h_t$ is the hidden state. The hidden state is used to store the information from previous state. $x_t$ is the input at time $t$. $y_t$ is the output at time $t$. 

The formula of RNN is shown as follows:
$$
h_t = f(W_{hh}h_{t-1} + W_{hx}x_t) 
$$

$$
y_t = f(W_{yh}h_t)
$$

The $f$ is the activation function, usually is sigmoid or tanh. The weight $W_{hh}, W_{hx}, W_{yh}$ are the weight that used to train the network.

The RNN can be used for many tasks, such as translation, text generation, etc.

**Comments: The RNN can not handle the long dependency. That means the output at time $t$ is only depend on the input at time $t$ and previous hidden state.**

To use RNN, we need to formula the problem as classification or regression problem, thus we have loss functions to train the network.

CBOW which introduce in last section is a formulated classification problem.

So we can used RNN as a network archtecture get the word embedding. 

Other than word embedding, the RNN can be used of a lot of tasks, for example, langauge translation. 

### Training
    I will write how to train RNN in the future.


## Temperature Sigmoid

Temperature is an important parater when talking about the sequence to sequence model. 
So, we first introduce it here before we go to the transformer.

In a tradtitonal lassification network, we use cross entropy to train the network and we hope the network confident for the class of the input image. 

The classification also used to train the sequence to sequence model. But when we want the model generate more creative content, we need the model generate the value other than the highest probability word. 

### How to Generate a word

In traditional classification network, we generate the tag use top1 or top5 tags. 
But if we use top1 in RNN, that everytime the generate sequence will be fix. That's say, one input only corresspond to one output.
But sometime, we want the model generate more than one results. For example, translation English to Chinese, there are more than one way to translate. 
The way to do this, is we use the probability of prediction and sample the word according to the probability. 
For example, the probability predict of word from a model after "I ate " is that apple: 0.1, orange: 0.8, bananay: 0.1; 
Instead generate orange diretely, we have 10%  probabilty to generate apple and banana, and 80% probability to generate orange.

As we known that, after training, the network have preference to generate some word, that was used to train the network.
So we use temeperature to make the netowrk more creative. 

Here is the way:
When calcualte the probability using sigmoid function $\frac{e^z}{1 + e^z}$, we replace it with  $\frac{e^{z/T}}{1 + e^{z/T}}$, where increase the temperature will increase the entorpy.

This simple mothod can encourage the network generate creative result. 



 -->
